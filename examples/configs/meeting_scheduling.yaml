simulation:
  max_iterations: 1
  max_planning_rounds: 1
  max_conversation_steps: 3
  seed: 42
  tags:
    - baseline
  note: "im a note"
environment:
  name: MeetingSchedulingEnvironment
  # Simple meeting scheduling (no CoLLAB dependency)
  num_meetings: 1 # Number of meetings to schedule
  timeline_length: 12 # Not used anymore (kept for compatibility)
  min_participants: 2 # Not used anymore
  max_participants: 4 # Not used anymore
  soft_meeting_ratio: 0.6 # Not used anymore
  # Availability table configuration
  num_days: 1 # Number of days to show in availability table
  slots_per_day: 6 # Number of time slots per day (reduced to fit 2048 token limit)
  availability_rate:
    0.4 # Probability that each agent is available for each slot (40% default)
    # OT protocol will discover actual intersection - NOT pre-determined!
communication_network:
  # Topology used to create blackboards
  # Options: complete | path | star | random
  # random requires `edge_prob` (0..1) and uses `simulation.seed`
  topology: random
  num_agents: 2
  edge_prob: .7
  # consolidate_channels=false => one blackboard per edge
  # consolidate_channels=true => one blackboard per clique (plus leftover edges)
  consolidate_channels: true
llm:
  # Provider: "openai", "anthropic", "gemini", "together", "vllm", "huggingface", or "mock"
  # COLAB Ä°Ã‡Ä°N: "huggingface" - Ãœcretsiz, API key gerektirmez, tool calling destekler!
  # NOT: GeliÅŸtirilmiÅŸ HuggingFaceClient artÄ±k tool calling'i JSON parsing ile destekliyor
  provider: "huggingface"

  # Hugging Face - ÃœCRETSÄ°Z + TOOL CALLING DESTEÄÄ°! ğŸ‰
  # Modeller otomatik olarak Hugging Face Hub'dan indirilir
  # Tool calling iÃ§in JSON parsing kullanÄ±lÄ±r (model'in structured output vermesi gerekir)
  huggingface:
    # Tool calling iÃ§in en iyi modeller (gÃ¼Ã§ sÄ±rasÄ±na gÃ¶re):
    
    # ğŸš€ COLAB T4 GPU Ä°Ã‡Ä°N EN Ä°YÄ° SEÃ‡Ä°M:
    model: "Qwen/Qwen2.5-7B-Instruct"  # 7B - MÃ¼kemmel denge (Ã–NERÄ°LEN!)
    
    # DiÄŸer gÃ¼Ã§lÃ¼ seÃ§enekler:
    # â­â­â­ "Qwen/Qwen2.5-14B-Instruct" - 14B - Ã‡ok gÃ¼Ã§lÃ¼ (Colab'da biraz yavaÅŸ ama Ã§alÄ±ÅŸÄ±r)
    # â­â­â­ "Qwen/Qwen2.5-32B-Instruct" - 32B - En gÃ¼Ã§lÃ¼ (Quantization gerekebilir)
    # â­â­ "meta-llama/Llama-3.1-8B-Instruct" - 8B - Meta'nÄ±n modeli
    # â­â­ "mistralai/Mistral-7B-Instruct-v0.3" - 7B - PopÃ¼ler, iyi
    # â­ "Qwen/Qwen2.5-3B-Instruct" - 3B - HÄ±zlÄ± ama daha az gÃ¼Ã§lÃ¼
    # â­ "Qwen/Qwen2.5-1.5B-Instruct" - 1.5B - En hÄ±zlÄ±, en hafif
    
    # MODEL BOYUTU REHBERÄ°:
    # 1.5B-3B: ~1-3GB VRAM, Ã§ok hÄ±zlÄ±, temel tasklar iÃ§in yeterli
    # 7B: ~7-8GB VRAM, mÃ¼kemmel denge, Ã§oÄŸu task iÃ§in ideal â­
    # 14B: ~14-16GB VRAM, Ã§ok iyi kalite, Colab T4'te sÄ±nÄ±rda
    # 32B+: ~32GB+ VRAM, en iyi kalite, quantization gerekir
    
    device: "auto" # "auto" (GPU varsa kullan), "cuda" (GPU zorla), "cpu" (CPU zorla)
    trust_remote_code: true
    params:
      max_tokens: 1024 # Yeterli token limiti (tool calls iÃ§in Ã¶nemli)
      temperature: 0.1 # DÃ¼ÅŸÃ¼k = daha yapÄ±landÄ±rÄ±lmÄ±ÅŸ, tutarlÄ± cevaplar

  openai:
    model: "gpt-4.1-mini-2025-04-14"
    params:
      max_tokens: 1024
      temperature: 0.7
      reasoning_effort: "low"
      verbosity: "low"
  anthropic:
    model: "claude-3-5-sonnet-20241022"
    params:
      max_tokens: 4000
      temperature: 0.7
      top_p: 0.95
  gemini:
    model: "gemini-2.0-flash-lite"
    params:
      max_tokens: 1024
      temperature: 0.6
      top_p: 0.95
  together:
    model: "zai-org/GLM-4.6"
    params:
      max_tokens: 1024
      temperature: 0.7
  vllm:
    auto_start_server: true
    persistent_server: false # Keep the vLLM server alive between runs to avoid cold starts
    health_check_path: "/health"
    startup_timeout: 600 # Increased for Colab (model download + loading)
    params:
      max_tokens: 512 # Yeterli token limiti
      temperature: 0.1 # DÃ¼ÅŸÃ¼k = daha kararlÄ± cevaplar
    models:
      - checkpoint: "Qwen/Qwen2.5-3B-Instruct" # Tool calling destekler
        host: "127.0.0.1"
        port: 8020
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.50 # Colab T4 iÃ§in gÃ¼venli (%50)
        max_model_len: 4096 # Yeterli context
        trust_remote_code: true
        tool_call_parser: "hermes" # Tool calling iÃ§in gerekli
        additional_args:
          - "--enforce-eager" # Colab iÃ§in gerekli
          - "--disable-frontend-multiprocessing"
          - "--enable-auto-tool-choice" # Tool calling'i aktifleÅŸtirir
notes:
  - "âœ… TOOL CALLING Ã‡ALIÅIYOR: HuggingFace provider artÄ±k JSON parsing ile tool calling destekliyor!"
  - "Provider 'huggingface' olarak ayarlanmÄ±ÅŸ - API key gerektirmez, tamamen Ã¼cretsiz!"
  - "Ä°lk Ã§alÄ±ÅŸtÄ±rmada model indirilir (~3GB Qwen 3B iÃ§in), sonraki Ã§alÄ±ÅŸtÄ±rmalar cache kullanÄ±r"
  - "GPU varsa otomatik kullanÄ±lÄ±r, yoksa CPU kullanÄ±lÄ±r (daha yavaÅŸ)"
  - "Model'in dÃ¼ÅŸÃ¼k temperature (0.1) ile structured output vermesi saÄŸlanÄ±yor"
  - "Qwen modelleri tool calling iÃ§in Ã¶zellikle iyi Ã§alÄ±ÅŸÄ±r"
  - "Alternatif: vLLM (daha hÄ±zlÄ± ama kurulumu karmaÅŸÄ±k), OpenAI/Anthropic (Ã¼cretli ama en iyi)"
  - "MeetingScheduling baseline configuration (no attacks)"
  - "MeetingScheduling always logs scores for each iteration"
  - "MeetingScheduling environment coordinates attendance intervals across agents"
  - "Agents coordinate via blackboards during planning phase"
  - "Final attendance decisions happen during execution using attend_meeting action"
  - "Meetings have fixed windows on a shared timeline; soft meetings reward overlap"
  - "Strict meetings reward fullâ€‘window attendance; overlaps across two meetings are penalized"
  - "Objective: Maximize global reward ratio from CoLLAB v2 scoring"
  - "Simulation stops when all attendance variables are assigned or max iterations reached"
