simulation:
  max_iterations: 1
  max_planning_rounds: 1
  max_conversation_steps: 3
  seed: 42
  tags:
    - baseline
  note: "im a note"
environment:
  name: MeetingSchedulingEnvironment
  # Simple meeting scheduling (no CoLLAB dependency)
  num_meetings: 1 # Number of meetings to schedule
  timeline_length: 12 # Not used anymore (kept for compatibility)
  min_participants: 2 # Not used anymore
  max_participants: 4 # Not used anymore
  soft_meeting_ratio: 0.6 # Not used anymore
  # Availability table configuration
  num_days: 1 # Number of days to show in availability table
  slots_per_day: 6 # Number of time slots per day (reduced to fit 2048 token limit)
  availability_rate:
    0.4 # Probability that each agent is available for each slot (40% default)
    # OT protocol will discover actual intersection - NOT pre-determined!
communication_network:
  # Topology used to create blackboards
  # Options: complete | path | star | random
  # random requires `edge_prob` (0..1) and uses `simulation.seed`
  topology: random
  num_agents: 2
  edge_prob: .7
  # consolidate_channels=false => one blackboard per edge
  # consolidate_channels=true => one blackboard per clique (plus leftover edges)
  consolidate_channels: true
llm:
  # Provider: "openai", "anthropic", "gemini", "together", "vllm", "huggingface", or "mock"
  # RECOMMENDED: "huggingface" - Ãœcretsiz, API key gerektirmez!
  provider: "huggingface"

  # Hugging Face - ÃœCRETSÄ°Z, API KEY GEREKTÄ°RMEZ! ðŸŽ‰
  # Modeller otomatik olarak Hugging Face Hub'dan indirilir
  # Ä°lk Ã§alÄ±ÅŸtÄ±rma yavaÅŸ (model indirme), sonraki Ã§alÄ±ÅŸtÄ±rmalar hÄ±zlÄ± (cache)
  huggingface:
    # Ã–nerilen modeller:
    # KÃ¼Ã§Ã¼k (hÄ±zlÄ±): "Qwen/Qwen2.5-1.5B-Instruct", "google/gemma-2b-it", "microsoft/phi-2"
    # Orta (dengeli): "Qwen/Qwen2.5-3B-Instruct", "meta-llama/Llama-3.2-3B-Instruct"
    # BÃ¼yÃ¼k (kaliteli): "Qwen/Qwen2.5-7B-Instruct", "mistralai/Mistral-7B-Instruct-v0.3"
    model: "Qwen/Qwen2.5-3B-Instruct"
    device: "auto" # "auto" (GPU varsa kullan), "cuda" (GPU zorla), "cpu" (CPU zorla)
    trust_remote_code: true
    params:
      max_tokens: 256
      temperature: 0.7

  openai:
    model: "gpt-4.1-mini-2025-04-14"
    params:
      max_tokens: 1024
      temperature: 0.7
      reasoning_effort: "low"
      verbosity: "low"
  anthropic:
    model: "claude-3-5-sonnet-20241022"
    params:
      max_tokens: 4000
      temperature: 0.7
      top_p: 0.95
  gemini:
    model: "gemini-2.0-flash-lite"
    params:
      max_tokens: 1024
      temperature: 0.6
      top_p: 0.95
  together:
    model: "zai-org/GLM-4.6"
    params:
      max_tokens: 1024
      temperature: 0.7
  vllm:
    auto_start_server: true
    persistent_server: false # Keep the vLLM server alive between runs to avoid cold starts
    health_check_path: "/models"
    startup_timeout: 600 # Increased for Colab (was 420)
    params:
      max_tokens: 256 # Reduced to fit in 2048 token context (1.5B model)
      temperature: 0.7
    models:
      - checkpoint: "Qwen/Qwen2.5-1.5B-Instruct"
        host: "127.0.0.1"
        port: 8020
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.70
        max_model_len: 2048
        trust_remote_code: true
        tool_call_parser: "hermes"
        additional_args:
          - "--enforce-eager" # Disable torch.compile to avoid Triton compilation errors
          - "--disable-frontend-multiprocessing"
notes:
  - "HUGGING FACE KULLANIMI: Provider 'huggingface' olarak ayarlanmÄ±ÅŸ - API key gerektirmez!"
  - "Ä°lk Ã§alÄ±ÅŸtÄ±rmada model indirilir (~1-7GB), sonraki Ã§alÄ±ÅŸtÄ±rmalar cache kullanÄ±r"
  - "GPU varsa otomatik kullanÄ±lÄ±r, yoksa CPU kullanÄ±lÄ±r (daha yavaÅŸ)"
  - "Model deÄŸiÅŸtirmek iÃ§in: llm.huggingface.model alanÄ±nÄ± dÃ¼zenleyin"
  - "DiÄŸer provider'larÄ± kullanmak iÃ§in: llm.provider deÄŸerini deÄŸiÅŸtirin (openai, anthropic, gemini, vllm)"
  - "MeetingScheduling baseline configuration (no attacks)"
  - "MeetingScheduling always logs scores for each iteration"
  - "MeetingScheduling environment coordinates attendance intervals across agents"
  - "Agents coordinate via blackboards during planning phase"
  - "Final attendance decisions happen during execution using attend_meeting action"
  - "Meetings have fixed windows on a shared timeline; soft meetings reward overlap"
  - "Strict meetings reward fullâ€‘window attendance; overlaps across two meetings are penalized"
  - "Objective: Maximize global reward ratio from CoLLAB v2 scoring"
  - "Simulation stops when all attendance variables are assigned or max iterations reached"
