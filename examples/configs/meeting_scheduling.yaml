simulation:
  max_iterations: 1
  max_planning_rounds: 1
  max_conversation_steps: 3
  seed: 42
  tags:
    - baseline
  note: "im a note"
environment:
  name: MeetingSchedulingEnvironment

  # ========== INTERSECTION CONTROL ==========
  # intersections: Number of GUARANTEED common slots where ALL agents are available
  # Example: intersections=3 means exactly 3 slots where both Agent A and Agent B are free
  # The OT protocol will discover these slots during execution (privacy-preserving!)
  intersections: 3 # GUARANTEED: Exactly 3 common available slots for all participants

  num_meetings: 1 # Number of meetings to schedule

  # ========== AVAILABILITY TABLE CONFIGURATION ==========
  num_days: 1 # Number of days to show in availability table
  slots_per_day: 6 # Number of time slots per day
  availability_rate: 0.4 # Probability that each agent is available for NON-INTERSECTION slots (40% default)
communication_network:
  # Topology used to create blackboards
  # Options: complete | path | star | random
  # random requires `edge_prob` (0..1) and uses `simulation.seed`
  topology: random
  num_agents: 2
  edge_prob: .7
  # consolidate_channels=false => one blackboard per edge
  # consolidate_channels=true => one blackboard per clique (plus leftover edges)
  consolidate_channels: true
llm:
  # Provider: "openai", "anthropic", "gemini", "together", "vllm", "huggingface", or "mock"
  # COLAB Ä°Ã‡Ä°N: "huggingface" - Ãœcretsiz, API key gerektirmez, tool calling destekler!
  # NOT: GeliÅŸtirilmiÅŸ HuggingFaceClient artÄ±k tool calling'i JSON parsing ile destekliyor
  provider: "huggingface"

  # Hugging Face - ÃœCRETSÄ°Z + TOOL CALLING DESTEÄÄ°! ğŸ‰
  # Modeller otomatik olarak Hugging Face Hub'dan indirilir
  # Tool calling iÃ§in JSON parsing kullanÄ±lÄ±r (model'in structured output vermesi gerekir)
  huggingface:
    # Tool calling iÃ§in en iyi modeller (gÃ¼Ã§ sÄ±rasÄ±na gÃ¶re):

    # ğŸš€ COLAB T4 GPU Ä°Ã‡Ä°N EN Ä°YÄ° SEÃ‡Ä°M:
    model: "Qwen/Qwen2.5-7B-Instruct" # 7B - MÃ¼kemmel denge (Ã–NERÄ°LEN!)

    # DiÄŸer gÃ¼Ã§lÃ¼ seÃ§enekler:
    # â­â­â­ "Qwen/Qwen2.5-14B-Instruct" - 14B - Ã‡ok gÃ¼Ã§lÃ¼ (Colab'da biraz yavaÅŸ ama Ã§alÄ±ÅŸÄ±r)
    # â­â­â­ "Qwen/Qwen2.5-32B-Instruct" - 32B - En gÃ¼Ã§lÃ¼ (Quantization gerekebilir)
    # â­â­ "meta-llama/Llama-3.1-8B-Instruct" - 8B - Meta'nÄ±n modeli
    # â­â­ "mistralai/Mistral-7B-Instruct-v0.3" - 7B - PopÃ¼ler, iyi
    # â­ "Qwen/Qwen2.5-3B-Instruct" - 3B - HÄ±zlÄ± ama daha az gÃ¼Ã§lÃ¼
    # â­ "Qwen/Qwen2.5-1.5B-Instruct" - 1.5B - En hÄ±zlÄ±, en hafif

    # MODEL BOYUTU REHBERÄ°:
    # 1.5B-3B: ~1-3GB VRAM, Ã§ok hÄ±zlÄ±, temel tasklar iÃ§in yeterli
    # 7B: ~7-8GB VRAM, mÃ¼kemmel denge, Ã§oÄŸu task iÃ§in ideal â­
    # 14B: ~14-16GB VRAM, Ã§ok iyi kalite, Colab T4'te sÄ±nÄ±rda
    # 32B+: ~32GB+ VRAM, en iyi kalite, quantization gerekir

    device: "auto" # "auto" (GPU varsa kullan), "cuda" (GPU zorla), "cpu" (CPU zorla)
    trust_remote_code: true
    params:
      max_tokens: 1024 # Yeterli token limiti (tool calls iÃ§in Ã¶nemli)
      temperature: 0.1 # DÃ¼ÅŸÃ¼k = daha yapÄ±landÄ±rÄ±lmÄ±ÅŸ, tutarlÄ± cevaplar

  openai:
    model: "gpt-4.1-mini-2025-04-14"
    params:
      max_tokens: 1024
      temperature: 0.7
      reasoning_effort: "low"
      verbosity: "low"
  anthropic:
    model: "claude-3-5-sonnet-20241022"
    params:
      max_tokens: 4000
      temperature: 0.7
      top_p: 0.95
  gemini:
    model: "gemini-2.0-flash-lite"
    params:
      max_tokens: 1024
      temperature: 0.6
      top_p: 0.95
  together:
    model: "zai-org/GLM-4.6"
    params:
      max_tokens: 1024
      temperature: 0.7
  vllm:
    auto_start_server: true
    persistent_server: false # Keep the vLLM server alive between runs to avoid cold starts
    health_check_path: "/health"
    startup_timeout: 600 # Increased for Colab (model download + loading)
    params:
      max_tokens: 512 # Yeterli token limiti
      temperature: 0.1 # DÃ¼ÅŸÃ¼k = daha kararlÄ± cevaplar
    models:
      - checkpoint: "Qwen/Qwen2.5-3B-Instruct" # Tool calling destekler
        host: "127.0.0.1"
        port: 8020
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.50 # Colab T4 iÃ§in gÃ¼venli (%50)
        max_model_len: 4096 # Yeterli context
        trust_remote_code: true
        tool_call_parser: "hermes" # Tool calling iÃ§in gerekli
        additional_args:
          - "--enforce-eager" # Colab iÃ§in gerekli
          - "--disable-frontend-multiprocessing"
          - "--enable-auto-tool-choice" # Tool calling'i aktifleÅŸtirir
notes:
  - "âœ… TOOL CALLING + VRAM OPTIMIZE: HuggingFace provider tool calling supports + model sharing active!"
  - "ğŸ”§ OOM FIX: All agents share the same model instance (VRAM savings)"
  - "Provider set to 'huggingface' - No API key required, completely free!"
  - "First run downloads model (~7GB for Qwen 7B), subsequent runs use cache"
  - "GPU automatically used if available, falls back to CPU (slower)"
  - "Model configured with low temperature (0.1) for structured output"
  - "Qwen 7B model excellent for tool calling, works with 2+ agents"
  - "If OOM occurs: use model: 'Qwen/Qwen2.5-3B-Instruct' or 1.5B"
  - "Alternatives: vLLM (faster but complex setup), OpenAI/Anthropic (paid but best)"
  - "MeetingScheduling baseline configuration"
  - "Agents coordinate via blackboards using privacy-preserving OT protocol"
  - "Environment tracks scores for each iteration"
  - "Objective: Schedule meetings at earliest common available slots"
  - "Simulation stops when all meetings scheduled or max iterations reached"
