simulation:
  max_iterations: 1
  max_planning_rounds: 1
  max_conversation_steps: 3
  seed: 42
  tags:
    - baseline
  note: "im a note"
environment:
  name: MeetingSchedulingEnvironment

  # ========== PRODUCTION MODE TOGGLE ==========
  # use_real_calendars: Use real Outlook/Teams via Microsoft Graph API
  # false = Simulation mode (default, no Azure setup needed)
  # true = Production mode (requires Azure App Registration + M365 account)
  use_real_calendars: false # âœ… PRODUCTION MODE (Graph API)

  # ========== MICROSOFT GRAPH API CONFIGURATION (Optional) ==========
  # Only used if use_real_calendars=true
  # Requires Azure App Registration: https://portal.azure.com
  # Free options:
  # - University M365 account (if your school provides it)
  # - M365 Developer Program: https://developer.microsoft.com/microsoft-365/dev-program (90 days free)
  graph_api:
    # Environment variables (NEVER commit secrets to git!)
    # Set these in your shell:
    #   $env:AZURE_CLIENT_ID="your-client-id"
    #   $env:AZURE_CLIENT_SECRET="your-secret-value"
    #   $env:AZURE_TENANT_ID="your-tenant-id"
    client_id: "${AZURE_CLIENT_ID}"
    client_secret: "${AZURE_CLIENT_SECRET}"
    tenant_id: "${AZURE_TENANT_ID}"

    # Timezone for calendar operations
    timezone: "Europe/Istanbul" # Adjust to your timezone

    # Map simulation agent names to real email addresses
    # Example: AgentA uses real Outlook calendar of user1@university.edu
    agent_emails:
      AgentA: "user1@example.com" # Replace with real email
      AgentB: "user2@example.com" # Replace with real email

  # ========== INTERSECTION CONTROL ==========
  # intersections: Number of GUARANTEED common slots where ALL agents are available
  # Example: intersections=3 means exactly 3 slots where both Agent A and Agent B are free
  # The OT protocol will discover these slots during execution (privacy-preserving!)
  intersections: 3 # GUARANTEED: Exactly 3 common available slots for all participants

  num_meetings: 1 # Number of meetings to schedule

  # ========== AVAILABILITY TABLE CONFIGURATION ==========
  # Only used in simulation mode (use_real_calendars=false)
  num_days: 5 # Number of days to show in availability table
  slots_per_day: 24 # Number of time slots per day
  availability_rate: 0.4 # %40 ihtimalle ek slotlar available (her agent FARKLI olacak!)
  # availability_rate: 0.0 # 0.0 = SADECE intersection slotlarÄ± (test iÃ§in)
communication_network:
  # Topology used to create blackboards
  # Options: complete | path | star | random
  # random requires `edge_prob` (0..1) and uses `simulation.seed`
  topology: random
  num_agents: 2
  edge_prob: .7
  # consolidate_channels=false => one blackboard per edge
  # consolidate_channels=true => one blackboard per clique (plus leftover edges)
  consolidate_channels: true
llm:
  # Provider: "openai", "anthropic", "gemini", "together", "vllm", "huggingface", or "mock"
  # COLAB Ä°Ã‡Ä°N: "huggingface" - Ãœcretsiz, API key gerektirmez, tool calling destekler!
  # NOT: GeliÅŸtirilmiÅŸ HuggingFaceClient artÄ±k tool calling'i JSON parsing ile destekliyor
  provider: "huggingface"

  # Hugging Face - ÃœCRETSÄ°Z + TOOL CALLING DESTEÄÄ°! ğŸ‰
  # Modeller otomatik olarak Hugging Face Hub'dan indirilir
  # Tool calling iÃ§in JSON parsing kullanÄ±lÄ±r (model'in structured output vermesi gerekir)
  huggingface:
    # ============================================
    # ğŸ¯ EN GÃœÃ‡LÃœ MODELLER (OpenAI GPT-4 Seviyesi)
    # ============================================
    # ğŸ”¥ "Qwen/Qwen2.5-72B-Instruct" - 72B - GPT-4'e en yakÄ±n! (~72GB VRAM, quantization ÅŸart)
    # ğŸ”¥ "meta-llama/Llama-3.3-70B-Instruct" - 70B - En yeni Llama, Ã§ok gÃ¼Ã§lÃ¼ (~70GB VRAM)
    # ğŸ”¥ "mistralai/Mixtral-8x22B-Instruct-v0.1" - 176B total (MoE), ultra gÃ¼Ã§lÃ¼ (~90GB VRAM)
    # ğŸ”¥ "Qwen/QwQ-32B-Preview" - 32B - Reasoning'de mÃ¼kemmel, math/logic iÃ§in ideal (~32GB VRAM)

    # ============================================
    # ğŸš€ BALANCED - COLAB T4 GPU Ä°Ã‡Ä°N EN Ä°YÄ°
    # ============================================
    model: "Qwen/Qwen2.5-7B-Instruct" # 7B - MÃ¼kemmel denge, tool calling mÃ¼kemmel (Ã–NERÄ°LEN!)

    # ============================================
    # ğŸ’ª GÃœÃ‡LÃœ MODELLER (GPT-3.5+ Seviyesi)
    # ============================================
    # â­â­â­â­ "Qwen/Qwen2.5-32B-Instruct" - 32B - Ã‡ok gÃ¼Ã§lÃ¼, GPT-3.5 Turbo'dan iyi (~32GB VRAM, quantization gerekebilir)
    # â­â­â­â­ "meta-llama/Llama-3.1-70B-Instruct" - 70B - Meta'nÄ±n en gÃ¼Ã§lÃ¼sÃ¼ (~70GB VRAM)
    # â­â­â­ "Qwen/Qwen2.5-14B-Instruct" - 14B - Ã‡ok iyi, Colab T4'te sÄ±nÄ±rda (~14GB VRAM)
    # â­â­â­ "mistralai/Mixtral-8x7B-Instruct-v0.1" - 47B total (MoE), Ã§ok gÃ¼Ã§lÃ¼ (~24GB VRAM)
    # â­â­ "meta-llama/Llama-3.1-8B-Instruct" - 8B - Meta'nÄ±n 8B versiyonu (~8GB VRAM)
    # â­â­ "mistralai/Mistral-7B-Instruct-v0.3" - 7B - PopÃ¼ler, iyi (~7GB VRAM)

    # ============================================
    # âš¡ HIZLI MODELLER (GPT-3.5 Mini Seviyesi)
    # ============================================
    # â­ "Qwen/Qwen2.5-3B-Instruct" - 3B - HÄ±zlÄ±, basic tasks iÃ§in yeterli (~3GB VRAM)
    # â­ "Qwen/Qwen2.5-1.5B-Instruct" - 1.5B - En hÄ±zlÄ±, en hafif (~2GB VRAM)

    # ============================================
    # ğŸ“Š VRAM GEREKSÄ°NÄ°MLERÄ° (FP16 ile)
    # ============================================
    # 1.5B-3B:    ~2-4GB    | Colab free âœ… | CPU âœ… | Tool calling: â­â­
    # 7B-8B:      ~7-10GB   | Colab T4 âœ…   | CPU âŒ | Tool calling: â­â­â­â­
    # 14B:        ~14-18GB  | Colab T4 âš ï¸   | CPU âŒ | Tool calling: â­â­â­â­â­
    # 32B:        ~32-40GB  | A100 40GB âœ…  | CPU âŒ | Tool calling: â­â­â­â­â­
    # 70B:        ~70-90GB  | A100 80GB âœ…  | CPU âŒ | Tool calling: â­â­â­â­â­
    #
    # ğŸ’¡ Quantization ile daha az VRAM: 4-bit quantization ~75% VRAM tasarrufu

    device: "auto" # "auto" (GPU varsa kullan), "cuda" (GPU zorla), "cpu" (CPU zorla)
    trust_remote_code: true
    params:
      max_tokens: 1024 # Yeterli token limiti (tool calls iÃ§in Ã¶nemli)
      temperature: 0.1 # DÃ¼ÅŸÃ¼k = daha yapÄ±landÄ±rÄ±lmÄ±ÅŸ, tutarlÄ± cevaplar

  openai:
    model: "gpt-4.1-mini-2025-04-14"
    params:
      max_tokens: 1024
      temperature: 0.7
      reasoning_effort: "low"
      verbosity: "low"
  anthropic:
    model: "claude-3-5-sonnet-20241022"
    params:
      max_tokens: 4000
      temperature: 0.7
      top_p: 0.95
  gemini:
    model: "gemini-2.0-flash-lite"
    params:
      max_tokens: 1024
      temperature: 0.6
      top_p: 0.95
  together:
    model: "zai-org/GLM-4.6"
    params:
      max_tokens: 1024
      temperature: 0.7
  vllm:
    auto_start_server: true
    persistent_server: false # Keep the vLLM server alive between runs to avoid cold starts
    health_check_path: "/health"
    startup_timeout: 600 # Increased for Colab (model download + loading)
    params:
      max_tokens: 512 # Yeterli token limiti
      temperature: 0.1 # DÃ¼ÅŸÃ¼k = daha kararlÄ± cevaplar
    models:
      - checkpoint: "Qwen/Qwen2.5-3B-Instruct" # Tool calling destekler
        host: "127.0.0.1"
        port: 8020
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.50 # Colab T4 iÃ§in gÃ¼venli (%50)
        max_model_len: 4096 # Yeterli context
        trust_remote_code: true
        tool_call_parser: "hermes" # Tool calling iÃ§in gerekli
        additional_args:
          - "--enforce-eager" # Colab iÃ§in gerekli
          - "--disable-frontend-multiprocessing"
          - "--enable-auto-tool-choice" # Tool calling'i aktifleÅŸtirir
notes:
  - "âœ… TOOL CALLING + VRAM OPTIMIZE: HuggingFace provider tool calling supports + model sharing active!"
  - "ğŸ”§ OOM FIX: All agents share the same model instance (VRAM savings)"
  - "Provider set to 'huggingface' - No API key required, completely free!"
  - "First run downloads model (~7GB for Qwen 7B), subsequent runs use cache"
  - "GPU automatically used if available, falls back to CPU (slower)"
  - "Model configured with low temperature (0.1) for structured output"
  - "Qwen 7B model excellent for tool calling, works with 2+ agents"
  - "If OOM occurs: use model: 'Qwen/Qwen2.5-3B-Instruct' or 1.5B"
  - "Alternatives: vLLM (faster but complex setup), OpenAI/Anthropic (paid but best)"
  - "MeetingScheduling baseline configuration"
  - "Agents coordinate via blackboards using privacy-preserving OT protocol"
  - "Environment tracks scores for each iteration"
  - "Objective: Schedule meetings at earliest common available slots"
  - "Simulation stops when all meetings scheduled or max iterations reached"
