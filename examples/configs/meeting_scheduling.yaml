simulation:
  max_iterations: 1
  max_planning_rounds: 1
  max_conversation_steps: 3
  seed: 42
  tags:
    - baseline
  note: "im a note"
environment:
  name: MeetingSchedulingEnvironment
  # Simple meeting scheduling (no CoLLAB dependency)
  num_meetings: 1 # Number of meetings to schedule
  timeline_length: 12 # Not used anymore (kept for compatibility)
  min_participants: 2 # Not used anymore
  max_participants: 4 # Not used anymore
  soft_meeting_ratio: 0.6 # Not used anymore
  # Availability table configuration
  num_days: 1 # Number of days to show in availability table
  slots_per_day: 12 # Number of time slots per day
  intersection: 3 # Number of common available slots (where all agents have 1)
communication_network:
  # Topology used to create blackboards
  # Options: complete | path | star | random
  # random requires `edge_prob` (0..1) and uses `simulation.seed`
  topology: random
  num_agents: 2
  edge_prob: .7
  # consolidate_channels=false => one blackboard per edge
  # consolidate_channels=true => one blackboard per clique (plus leftover edges)
  consolidate_channels: true
llm:
  # Provider: "openai", "anthropic", "gemini", "together", "vllm", or "mock"
  provider: "vllm" # Using Qwen2.5-3B-Instruct on Colab
  openai:
    model: "gpt-4.1-mini-2025-04-14"
    params:
      max_tokens: 1024
      temperature: 0.7
      reasoning_effort: "low"
      verbosity: "low"
  anthropic:
    model: "claude-3-5-sonnet-20241022"
    params:
      max_tokens: 4000
      temperature: 0.7
      top_p: 0.95
  gemini:
    model: "gemini-2.0-flash-lite"
    params:
      max_tokens: 1024
      temperature: 0.6
      top_p: 0.95
  together:
    model: "zai-org/GLM-4.6"
    params:
      max_tokens: 1024
      temperature: 0.7
  vllm:
    auto_start_server: true
    persistent_server: false # Keep the vLLM server alive between runs to avoid cold starts
    health_check_path: "/models"
    startup_timeout: 600 # Increased for Colab (was 420)
    params:
      max_tokens: 1024
      temperature: 0.7
    models:
      - checkpoint: "Qwen/Qwen2.5-3B-Instruct"
        host: "127.0.0.1"
        port: 8020
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.70
        max_model_len: 4096
        trust_remote_code: true
        tool_call_parser: "hermes"
        additional_args: []
notes:
  - MeetingScheduling baseline configuration (no attacks)
  - MeetingScheduling always logs scores for each iteration
  - MeetingScheduling environment coordinates attendance intervals across agents
  - Agents coordinate via blackboards during planning phase
  - Final attendance decisions happen during execution using attend_meeting action
  - Meetings have fixed windows on a shared timeline; soft meetings reward overlap
  - Strict meetings reward fullâ€‘window attendance; overlaps across two meetings are penalized
  - Objective: Maximize global reward ratio from CoLLAB v2 scoring
  - Simulation stops when all attendance variables are assigned or max iterations reached
